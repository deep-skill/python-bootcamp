{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# La forma de la función\n",
    "\n",
    "Como ya mencionamos, resolver un problema de regresión sin asumir antes la forma de la función puede ser un reto muy complicado. Sin embargo, hacer una incorrecta asumpción nos puede llevar a un modelo de regresión inexacto.\n",
    "\n",
    "Veamos la siguiente tira cómica acerca de las distintas formas que puede tener la función en un mismo conjunto de datos\n",
    "\n",
    "<img src=\"Images/curve1.jpg\" alt=\"Notas\" width=\"600\" center/>\n",
    "<img src=\"Images/curve2.jpg\" alt=\"Notas\" width=\"600\" center/>\n",
    "\n",
    "Observamos que si la forma de la función es muy simple (por ejemplo, lineal), la función ajustada no es muy capaz de capturar la verdadera naturaleza de los datos. Sin embargo, si la forma de la función se asume que es muy compleja, la función intentará ir por cada punto de los datos, lo cual no es ideal pues los datos pueden contener ruido.\n",
    "\n",
    "Por lo que, escoger la forma de la función puede ser complicado y requiere tener un conocimiento previo acerca de la función o el problema. Si la relación entre $x$ e $y$ es sabido que es lineal, entonces una forma de función lineal debe ser escogida. Por otra parte, si la relación es sabido que es no lineal pero la forma exacta es desconocida, entonces $x$ e $y$ deben ser dibujadas primero para tener una idea de la forma de la función. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresión mínimos cuadrados\n",
    "\n",
    "Un método muy común para resolver el problema de la regresión es llamado **regresión mínimos cuadrados** (LSE por sus siglas en inglés). En LSE, el problema de minimización se resuelve dando a la gradiente del objetivo igual a cero, es decir\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\theta_j}\\sum_i (y_i-f(x_i;\\theta))^2 = 0, \n",
    "$$\n",
    "\n",
    "para cada $j$. \n",
    "\n",
    "**Regresión mínimos cuadrados lineal:** Si la función es lineal, es decir $f(x_i;\\theta) = \\theta_1 x_i + \\theta_2$, dando la gradiente igual a cero nos provee un conjunto de ecuaciones para cada $\\theta_j$. Si el número de puntos $(x_i, y_i)$ es mayor que el número de variables, entonces estas ecuaciones se pueden resolver fácilmente y encontrar $\\theta$ que minimize el objetivo. Aquí puede encontrar una referencia más profunda sobre este método y su solución: ([Link](https://textbooks.math.gatech.edu/ila/least-squares.html)).\n",
    "\n",
    "**Regresión mínimos cuadrados no lineal:** Cuando la función es no lineal, una solución puede ser obtenida con un método iterativo haciendo una aproximación lineal en cada paso usando el llamado *método Gauss Newton*."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
